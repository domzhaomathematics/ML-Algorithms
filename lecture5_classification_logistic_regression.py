# -*- coding: utf-8 -*-
"""Lecture5:Classification_Logistic_Regression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RM54lEXe4UYtAR58AJmI-lIxXAHkqY2F
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print("ok")

url="https://raw.githubusercontent.com/epfml/ML_course/master/labs/ex05/template/height_weight_genders.csv"
data=pd.read_csv(url)
print(data.head())

"""**Helper functions**"""

def standardize(x):
    """Standardize the original data set."""
    mean_x = np.mean(x, axis=0)
    x = x - mean_x
    std_x = np.std(x, axis=0)
    x = x / std_x
    return x, mean_x, std_x


def de_standardize(x, mean_x, std_x):
    """Reverse the procedure of standardization."""
    x = x * std_x
    x = x + mean_x
    return x


def build_model_data(height, weight):
    """Form (y,tX) to get regression data in matrix form."""
    y = weight
    x = height
    num_samples = len(y)
    tx = np.c_[np.ones(num_samples), x]
    return y, tx

def load_data(data,sub_sample=True, add_outlier=False):
    height = data.iloc[:, 1]
    weight = data.iloc[:, 2]
    gender=data.iloc[:,0]
    # Convert to metric system
    height *= 0.025
    weight *= 0.454
    return height, weight, gender
def sample_data(y, x, seed, size_samples):
    """sample from dataset."""
    np.random.seed(seed)
    num_observations = y.shape[0]
    random_permuted_indices = np.random.permutation(num_observations)
    y = y[random_permuted_indices]
    x = x[random_permuted_indices]
    return y[:size_samples], x[:size_samples]

height,weight,gender=load_data(data)
new_array=[]
for row in gender:
  if row=="Male":
    new_array.append(0)
  else:
    new_array.append(1)
gender=pd.Series(new_array)
height=np.array(height)
weight=np.array(weight)
print(gender.head())

def least_squares(y,tx):
  #this function will give the optimal solution
  a=tx.T.dot(tx)
  b=tx.T.dot(y)

  return np.linalg.solve(a,b)

"""**Plots**"""

def visualization(y, x, mean_x, std_x, w, save_name):
    """visualize the raw data as well as the classification result."""
    fig = plt.figure()
    # plot raw data
    x = de_standardize(x, mean_x, std_x)
    ax1 = fig.add_subplot(1, 2, 1)
    males = np.where(y == 1)
    females = np.where(y == 0)
    ax1.scatter(
        x[males, 0], x[males, 1],
        marker='.', color=[0.06, 0.06, 1], s=20)
    ax1.scatter(
        x[females, 0], x[females, 1],
        marker='*', color=[1, 0.06, 0.06], s=20)
    ax1.set_xlabel("Height")
    ax1.set_ylabel("Weight")
    ax1.grid()
    # plot raw data with decision boundary
    ax2 = fig.add_subplot(1, 2, 2)
    height = np.arange(
        np.min(x[:, 0]), np.max(x[:, 0]) + 0.01, step=0.01)
    weight = np.arange(
        np.min(x[:, 1]), np.max(x[:, 1]) + 1, step=1)
    hx, hy = np.meshgrid(height, weight)
    hxy = (np.c_[hx.reshape(-1), hy.reshape(-1)] - mean_x) / std_x
    x_temp = np.c_[np.ones((hxy.shape[0], 1)), hxy]
    prediction = x_temp.dot(w) > 0.5
    prediction = prediction.reshape((weight.shape[0], height.shape[0]))
    ax2.contourf(hx, hy, prediction, 1)
    ax2.scatter(
        x[males, 0], x[males, 1],
        marker='.', color=[0.06, 0.06, 1], s=20)
    ax2.scatter(
        x[females, 0], x[females, 1],
        marker='*', color=[1, 0.06, 0.06], s=20)
    ax2.set_xlabel("Height")
    ax2.set_ylabel("Weight")
    ax2.set_xlim([min(x[:, 0]), max(x[:, 0])])
    ax2.set_ylim([min(x[:, 1]), max(x[:, 1])])
    plt.tight_layout()
    plt.savefig(save_name)

"""**Classification using linear regression**"""

seed=1
y = np.expand_dims(gender, axis=1)
X = np.c_[height.reshape(-1), weight.reshape(-1)]
y, X = sample_data(y, X, seed, size_samples=200)
x, mean_x, std_x = standardize(X)

def least_square_classification_demo(y, x):
    # classify the data by linear regression
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = least_squares(y, tx)

    # visualize your classification.
    visualization(y, x, mean_x, std_x, w, "classification_by_least_square")
    
least_square_classification_demo(y, x)

"""**Exercice 2: Logistic Regression**"""

def sigmoid(t):
  return np.exp(t)/(1+np.exp(t))

def calculate_loss(y, tx, w):
  #loss=np.sum(np.log(np.ones(y.size)+np.exp(tx.dot(w)))-tx.dot(w).dot(y.T))
  pred = sigmoid(tx.dot(w))
  #loss = y.T.dot(np.log(pred)) + (1 - y).T.dot(np.log(1 - pred))
  loss=np.sum(np.log(1+np.exp(tx.dot(w))))-y.T.dot(tx.dot(w))
  return loss

  #return loss

def learning_by_gradient_descent(y, tx, w, gamma):
  gradient=tx.T.dot(sigmoid(tx.dot(w))-y)
  w=w-gamma*gradient
  loss=calculate_loss(y,tx,w)
  return loss, w

def logistic_regression_gradient_descent_demo(y, x):
    # init parameters
    max_iter = 10000
    threshold = 1e-8
    gamma = 0.01
    losses = []

    # build tx
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = np.zeros((tx.shape[1], 1))

    # start the logistic regression
    for iter in range(max_iter):
        # get loss and update w.
        loss, w = learning_by_gradient_descent(y, tx, w, gamma)
        # log info
        if iter % 100 == 0:
            print("Current iteration={i}, loss={l}".format(i=iter, l=loss))
        losses.append(loss)
        # converge criterion        
        losses.append(loss)
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break
    # visualization
    visualization(y, x, mean_x, std_x, w, "classification_by_logistic_regression_gradient_descent")
    print("loss={l}".format(l=calculate_loss(y, tx, w)))
   

logistic_regression_gradient_descent_demo(y, x)

"""**Exercice 3: Newton's method**"""

def calculate_hessian(y, tx, w):
  S=np.zeros((y.size,y.size))
  for i in range(y.size):
    S[i][i]=sigmoid((tx[i]).T.dot(w))*(1-sigmoid((tx[i]).T.dot(w)))
  return tx.T.dot(S).dot(tx)

def logistic_regression(y, tx, w):
    """return the loss, gradient, and hessian."""
    gradient=tx.T.dot(sigmoid(tx.dot(w))-y)
    return calculate_loss(y,tx,w),gradient,calculate_hessian(y,tx,w)
    # ***************************************************
    # INSERT YOUR CODE HERE
    # return loss, gradient, and hessian: TODO
    # ***************************************************

def learning_by_newton_method(y, tx, w):
  
  loss,gradient,hessian=logistic_regression(y,tx,w)
  w-=(np.linalg.inv(hessian).dot(gradient))
   
  return loss, w

def logistic_regression_newton_method_demo(y, x):
    # init parameters
    max_iter = 100
    threshold = 1e-8
    lambda_ = 0.1
    losses = []

    # build tx
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = np.zeros((tx.shape[1], 1))

    # start the logistic regression
    for iter in range(max_iter):
        # get loss and update w.
        loss, w = learning_by_newton_method(y, tx, w)
        # log info
        if iter % 1 == 0:
            print("Current iteration={i}, the loss={l}".format(i=iter, l=loss))
        # converge criterion
        losses.append(loss)
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break
    # visualization
    visualization(y, x, mean_x, std_x, w, "classification_by_logistic_regression_newton_method")
    print("loss={l}".format(l=calculate_loss(y, tx, w)))

logistic_regression_newton_method_demo(y, x)

"""**Penalized logistic regression**"""

def penalized_logistic_regression(y, tx, w, lambda_):
  loss=calculate_loss(y,tx,w)+lambda_*0.5*(w.T.dot(w))
  gradient=tx.T.dot(sigmoid(tx.dot(w))-y)+lambda_*w
  hessian=calculate_hessian(y,tx,w)+2*np.identity(w.size)
  
  return loss,gradient,hessian

def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):
  loss,gradient,hessian=penalized_logistic_regression(y,tx,w,lambda_)
  w-=gamma*((np.linalg.inv(hessian)).dot(gradient))
    
  return loss, w

def logistic_regression_penalized_gradient_descent_demo(y, x):
    # init parameters
    max_iter = 10000
    gamma = 0.01
    lambda_ = 0.1
    threshold = 1e-8
    losses = []

    # build tx
    tx = np.c_[np.ones((y.shape[0], 1)), x]
    w = np.zeros((tx.shape[1], 1))

    # start the logistic regression
    for iter in range(max_iter):
        # get loss and update w.
        loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)
        # log info
        if iter % 100 == 0:
            print("Current iteration={i}, loss={l}".format(i=iter, l=loss))
        # converge criterion
        losses.append(loss)
        if len(losses) > 1 and np.abs(losses[-1] - losses[-2]) < threshold:
            break
    # visualization
    visualization(y, x, mean_x, std_x, w, "classification_by_logistic_regression_penalized_gradient_descent")
    print("loss={l}".format(l=calculate_loss(y, tx, w)))
    
logistic_regression_penalized_gradient_descent_demo(y, x)

