# -*- coding: utf-8 -*-
"""Lecture3&4-Least_squares-Max_likelihood-overfiiting&Crossvalidation_BiasVariance_decomposition

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IK2A2jLuUF72ZbfHHrELUuAY1gVKpSpz
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

print("ok")

url="https://raw.githubusercontent.com/epfml/ML_course/master/labs/ex03/template/dataEx3.csv"
data=pd.read_csv(url)
print(data.head)

url="https://raw.githubusercontent.com/epfml/ML_course/master/labs/ex02/template/height_weight_genders.csv"
last_data=pd.read_csv(url)
print(last_data.head)

"""**Helper functions**"""

def load_data():
    """load data."""
    data = np.loadtxt("dataEx3.csv", delimiter=",", skiprows=1, unpack=True)
    x = data[0]
    y = data[1]
    return x, y


def load_data_from_ex02(sub_sample=True, add_outlier=False):
    """Load data and convert it to the metric system."""
    path_dataset = "height_weight_genders.csv"
    data = np.genfromtxt(
        path_dataset, delimiter=",", skip_header=1, usecols=[1, 2])
    height = data[:, 0]
    weight = data[:, 1]
    gender = np.genfromtxt(
        path_dataset, delimiter=",", skip_header=1, usecols=[0],
        converters={0: lambda x: 0 if b"Male" in x else 1})
    # Convert to metric system
    height *= 0.025
    weight *= 0.454

    # sub-sample
    if sub_sample:
        height = height[::50]
        weight = weight[::50]

    if add_outlier:
        # outlier experiment
        height = np.concatenate([height, [1.1, 1.2]])
        weight = np.concatenate([weight, [51.5/0.454, 55.3/0.454]])

    return height, weight, gender


def standardize(x):
    """Standardize the original data set."""
    mean_x = np.mean(x)
    x = x - mean_x
    std_x = np.std(x)
    x = x / std_x
    return x, mean_x, std_x


def build_model_data(height, weight):
    """Form (y,tX) to get regression data in matrix form."""
    y = weight
    x = height
    num_samples = len(y)
    tx = np.c_[np.ones(num_samples), x]
    return y, tx



"""**Least squares**"""

def least_squares(y,tx):
  #this function will give the optimal solution
  a=tx.T.dot(tx)
  b=tx.T.dot(y)

  return np.linalg.solve(a,b)

def rmse(y,tx,w):
  e=y-tx.dot(w)
  loss=(1/2)*np.mean(e**2)
 
  return np.sqrt(2*loss)

"""**Exercice 2**"""

def build_poly(x, degree):
    """polynomial basis functions for input data x, for j=0 up to j=degree."""
    poly = np.ones((len(x), 1))
    for deg in range(1, degree+1):
        poly = np.c_[poly, np.power(x, deg)]
    return poly

"""**Plots**"""

def plot_fitted_curve(y, x, weights, degree, ax):
    """plot the fitted curve."""
    ax.scatter(x, y, color='b', s=12, facecolors='none', edgecolors='r')
    xvals = np.arange(min(x) - 0.1, max(x) + 0.1, 0.1)
    tx = build_poly(xvals, degree)
    f = tx.dot(weights)
    ax.plot(xvals, f)
    ax.set_xlabel("x")
    ax.set_ylabel("y")
    ax.set_title("Polynomial degree " + str(degree))

def plot_train_test(train_errors, test_errors, lambdas, degree):
    """
    train_errors, test_errors and lambas should be list (of the same size) the respective train error and test error for a given lambda,
    * lambda[0] = 1
    * train_errors[0] = RMSE of a ridge regression on the train set
    * test_errors[0] = RMSE of the parameter found by ridge regression applied on the test set
    
    degree is just used for the title of the plot.
    """
    plt.semilogx(lambdas, train_errors, color='b', marker='*', label="Train error")
    plt.semilogx(lambdas, test_errors, color='r', marker='*', label="Test error")
    plt.xlabel("lambda")
    plt.ylabel("RMSE")
    plt.title("Ridge regression for polynomial degree " + str(degree))
    leg = plt.legend(loc=1, shadow=True)
    leg.draw_frame(False)
    plt.savefig("ridge_regression")

fig, ax = plt.subplots()
degree=11
x=data.iloc[:,0]
y=data.iloc[:,1]
tx=build_poly(x,degree)
weights=least_squares(y,tx)
plot_fitted_curve(data.iloc[:,1],data.iloc[:,0],weights,degree,ax)



"""**Exercice 3 : Train and Test**"""

def function_split_data(data,ratio):
  train=data.iloc[0:int(data.shape[0]*ratio),:]
  test=data.iloc[int(data.shape[0]*ratio):,:]
  print('done')
  
  return train,test

def train_test_split_demo(data,degree,ratio,ax):
  train,test=function_split_data(data,ratio)
  x_train=train.iloc[:,0]
  y_train=train.iloc[:,1]
  tx_train=build_poly(x_train,degree)
  weights=least_squares(y_train,tx_train)
  
  #for test
  x_test=test.iloc[:,0]
  y_test=test.iloc[:,1]
  tx_test=build_poly(x_test,degree)
  
  #plot figure for the train
  plot_fitted_curve(data.iloc[:,1],data.iloc[:,0],weights,degree,ax)
  

  
  tx_test=build_poly(x_test,degree)
  
  print("rmse with ",ratio*100,"% training with degree",
       degree," is: ")
  print(rmse(y_test,tx_test,weights))

fig, ax = plt.subplots()
train_test_split_demo(data,12,0.9,ax)

"""**Ridge Regression**"""

def ridge_regression(y, tx, lambda_):
    """implement ridge regression."""
    # ***************************************************
    # INSERT YOUR CODE HERE
    a=tx.T.dot(tx)+lambda_*2*len(y)*np.identity(tx.shape[1])
    b=tx.T.dot(y)
    
    return np.linalg.solve(a,b)
    # ridge regression: TODO
    # ***************************************************
    #raise NotImplementedError

def ridge_regression_demo(data, degree, ratio, seed):
    """ridge regression demo."""
    # define parameter
    lambdas = np.logspace(-5, 0, 15)
    # ***************************************************
    # INSERT YOUR CODE HERE
    # split the data, and return train and test data: TODO
    train,test=function_split_data(data,ratio)
    # ***************************************************
    #raise NotImplementedError
    # ***************************************************
    # INSERT YOUR CODE HERE
    # form train and test data with polynomial basis function: TODO
    x_train=train.iloc[:,0]
    y_train=train.iloc[:,1]
    tx_train=build_poly(x_train,degree)
    
    x_test=test.iloc[:,0]
    y_test=test.iloc[:,1]
    tx_test=build_poly(x_test,degree)
    # ***************************************************
    #raise NotImplementedError
    

    rmse_tr = []
    rmse_te = []
    for ind, lambda_ in enumerate(lambdas):
        # ***************************************************
        # INSERT YOUR CODE HERE
        # ridge regression with a given lambda
        weights=ridge_regression(y_train,tx_train,lambda_)
        rmse_tr.append(rmse(y_train,tx_train,weights))
        rmse_te.append(rmse(y_test,tx_test,weights))
        
        
        # ***************************************************
        print("proportion={p}, degree={d}, lambda={l:.3f}, Training RMSE={tr:.3f}, Testing RMSE={te:.3f}".format(
               p=ratio, d=degree, l=lambda_, tr=rmse_tr[ind], te=rmse_te[ind]))
        
    # Plot the obtained results
    plot_train_test(rmse_tr, rmse_te, lambdas, degree)

    #raise NotImplementedError

seed = 56
degree = 7
split_ratio = 0.5
ridge_regression_demo(data, degree, split_ratio, seed)

"""## **Start of Lab 4**

**Exercice 1:  Cross-Validation**
"""

def cross_validation_visualization(lambds, mse_tr, mse_te):
    """visualization the curves of mse_tr and mse_te."""
    plt.semilogx(lambds, mse_tr, marker=".", color='b', label='train error')
    plt.semilogx(lambds, mse_te, marker=".", color='r', label='test error')
    plt.xlabel("lambda")
    plt.ylabel("rmse")
    plt.title("cross validation")
    plt.legend(loc=2)
    plt.grid(True)
    plt.savefig("cross_validation")

def build_k_indices(y, k_fold, seed):
    """build k indices for k-fold."""
    num_row = y.shape[0]
    interval = int(num_row / k_fold)
    np.random.seed(seed)
    indices = np.random.permutation(num_row)
    k_indices = [indices[k * interval: (k + 1) * interval]
                 for k in range(k_fold)]
    return np.array(k_indices)

def cross_validation(data, k_indices, lambda_, degree):
  rmse_te_temp=[]
  rmse_tr_temp=[]
  for k in k_indices:
  
        k_train=np.delete(k_indices,np.where(k_indices==k))
        train=data.iloc[k_train,:]
        test=data.iloc[k,:]
        x_train=train.iloc[:,0]
        y_train=train.iloc[:,1]
        tx_train=build_poly(x_train,degree)
    
        x_test=test.iloc[:,0]
        y_test=test.iloc[:,1]
        tx_test=build_poly(x_test,degree)
        
        weights=ridge_regression(y_train,tx_train,lambda_)
        rmse_tr_temp.append(rmse(y_train,tx_train,weights))
        rmse_te_temp.append(rmse(y_test,tx_test,weights))
        
        return np.mean(rmse_tr_temp), np.mean(rmse_te_temp)

def cross_validation_demo():
    seed = 1
    degree = 7
    k_fold = 4
    lambdas = np.logspace(-4, 0, 30)
    # split data in k fold
    k_indices = build_k_indices(y, k_fold, seed)
    # define lists to store the loss of training data and test data
    rmse_tr = []
    rmse_te = []
    # ***************************************************
    # INSERT YOUR CODE HERE
    # cross validation: TODO
    for lambda_ in lambdas:
      rmse_tr_1,rmse_te_1=cross_validation(data,k_indices,lambda_,degree)
      rmse_tr.append(rmse_tr_1)
      rmse_te.append(rmse_te_1)
      
      
     
        
        
      
      
    # ***************************************************    
    cross_validation_visualization(lambdas, rmse_tr, rmse_te)

cross_validation_demo()

"""**Exercice2: Bias-Variance Decomposition**"""

def bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te):
    """visualize the bias variance decomposition."""
    rmse_tr_mean = np.expand_dims(np.mean(rmse_tr, axis=0), axis=0)
    rmse_te_mean = np.expand_dims(np.mean(rmse_te, axis=0), axis=0)
    plt.plot(
        degrees,
        rmse_tr.T,
        'b',
        linestyle="-",
        color=([0.7, 0.7, 1]),
        label='train',
        linewidth=0.3)
    plt.plot(
        degrees,
        rmse_te.T,
        'r',
        linestyle="-",
        color=[1, 0.7, 0.7],
        label='test',
        linewidth=0.3)
    plt.plot(
        degrees,
        rmse_tr_mean.T,
        'b',
        linestyle="-",
        label='train',
        linewidth=3)
    plt.plot(
        degrees,
        rmse_te_mean.T,
        'r',
        linestyle="-",
        label='test',
        linewidth=3)
    plt.ylim(0.2, 0.7)
    plt.xlabel("degree")
    plt.ylabel("error")
    plt.title("Bias-Variance Decomposition")
    plt.savefig("bias_variance")

def bias_variance_demo():
    """The entry."""
    # define parameters
    seeds = range(100)
    num_data = 10000
    ratio_train = 0.05
    degrees = range(1, 15)
    lambda_=1
    # define list to store the variable
    rmse_tr = np.empty((len(seeds), len(degrees)))
    rmse_te = np.empty((len(seeds), len(degrees)))
    
    for index_seed, seed in enumerate(seeds):
        np.random.seed(seed)
        x = np.linspace(0.1, 2 * np.pi, num_data)
        y = np.sin(x) + 0.3 * np.random.randn(num_data).T
        # ***************************************************
        # INSERT YOUR CODE HERE
        # split data with a specific seed: TODO
        # ***************************************************
        #aise NotImplementedError
        data=pd.concat([pd.Series(x),pd.Series(y)],axis=1)
        train,test=function_split_data(data,1-ratio_train)
        
        for index_degree, degree in enumerate(degrees):
        
          x_train=train.iloc[:,0]
          y_train=train.iloc[:,1]
          tx_train=build_poly(x_train,degree)
    
          x_test=test.iloc[:,0]
          y_test=test.iloc[:,1]
          tx_test=build_poly(x_test,degree)
          
          weights=least_squares(y_train,tx_train)
          rmse_tr[index_seed,index_degree]=rmse(y_train,tx_train,weights)
          rmse_te[index_seed,index_degree]=rmse(y_test,tx_test,weights)
          
        # ***************************************************
        # INSERT YOUR CODE HERE
        # bias_variance_decomposition: TODO
        # ***************************************************
        #raise NotImplementedError

    bias_variance_decomposition_visualization(degrees, rmse_tr, rmse_te)

bias_variance_demo()

